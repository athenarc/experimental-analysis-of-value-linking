{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0476e574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value linkning dataset has been saved to assets/value_linking_dataset_bird_dev.json\n"
     ]
    }
   ],
   "source": [
    "from create_value_linking_dataset import ValueLinkingDatasetProcessor\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open(\"assets/dev_20240627/dev.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"assets/dev_20240627/dev_tables.json\", \"r\") as f:\n",
    "    schema_data = json.load(f)\n",
    "processor = ValueLinkingDatasetProcessor(schema_data)\n",
    "\n",
    "\n",
    "results = []\n",
    "for query in train_data:\n",
    "    sql_query = query[\"SQL\"]\n",
    "    db_id = query[\"db_id\"]\n",
    "    # Extract tables, columns, and values for the SQL query\n",
    "    tables, columns, values = processor.extract_tables_columns_and_values(sql_query, db_id)\n",
    "    results.append({\n",
    "        \"question\": query[\"question\"],\n",
    "        \"SQL\": sql_query,\n",
    "        \"tables\": tables,\n",
    "        \"columns\": columns,\n",
    "        \"values\": values,\n",
    "        \"db_id\": db_id,\n",
    "        \"source\": \"bird_dev\",\n",
    "        \"evidence\" : query[\"evidence\"]\n",
    "    })\n",
    "\n",
    "# Save results to a JSON file\n",
    "output_file_path = \"assets/value_linking_dataset_bird_dev.json\"\n",
    "with open(output_file_path, \"w\") as outfile:\n",
    "    json.dump(results, outfile, indent=4)\n",
    "    \n",
    "print(f\"Value linkning dataset has been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff0f513c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value linkning dataset has been saved to assets/value_linking_dataset_spider_dev.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"assets/spider_data/dev.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"assets/spider_data/tables.json\", \"r\") as f:\n",
    "    schema_data = json.load(f)\n",
    "# Initialize the SQLQueryProcessor with schema data\n",
    "processor = ValueLinkingDatasetProcessor(schema_data)\n",
    "\n",
    "\n",
    "results = []\n",
    "for query in train_data:\n",
    "    sql_query = query[\"query\"]\n",
    "    db_id = query[\"db_id\"]\n",
    "    # Extract tables, columns, and values for the SQL query\n",
    "    tables, columns, values = processor.extract_tables_columns_and_values(sql_query, db_id)\n",
    "    results.append({\n",
    "        \"question\": query[\"question\"],\n",
    "        \"SQL\": sql_query,\n",
    "        \"tables\": tables,\n",
    "        \"columns\": columns,\n",
    "        \"values\": values,\n",
    "        \"db_id\": db_id,\n",
    "        \"source\": \"spider_dev\",\n",
    "        \"evidence\" : \"\"\n",
    "    })\n",
    "\n",
    "# Save results to a JSON file\n",
    "output_file_path = \"assets/value_linking_dataset_spider_dev.json\"\n",
    "with open(output_file_path, \"w\") as outfile:\n",
    "    json.dump(results, outfile, indent=4)\n",
    "    \n",
    "print(f\"Value linkning dataset has been saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4caacf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value linkning dataset has been saved to assets/value_linking_dataset_spider_test.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"assets/spider_data/test.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"assets/spider_data/test_tables.json\", \"r\") as f:\n",
    "    schema_data = json.load(f)\n",
    "# Initialize the SQLQueryProcessor with schema data\n",
    "processor = ValueLinkingDatasetProcessor(schema_data)\n",
    "\n",
    "\n",
    "results = []\n",
    "for query in train_data:\n",
    "    sql_query = query[\"query\"]\n",
    "    db_id = query[\"db_id\"]\n",
    "    # Extract tables, columns, and values for the SQL query\n",
    "    tables, columns, values = processor.extract_tables_columns_and_values(sql_query, db_id)\n",
    "    results.append({\n",
    "        \"question\": query[\"question\"],\n",
    "        \"SQL\": sql_query,\n",
    "        \"tables\": tables,\n",
    "        \"columns\": columns,\n",
    "        \"values\": values,\n",
    "        \"db_id\": db_id,\n",
    "        \"source\": \"spider_test\",\n",
    "        \"evidence\" : \"\"\n",
    "    })\n",
    "\n",
    "# Save results to a JSON file\n",
    "output_file_path = \"assets/value_linking_dataset_spider_test.json\"\n",
    "with open(output_file_path, \"w\") as outfile:\n",
    "    json.dump(results, outfile, indent=4)\n",
    "    \n",
    "print(f\"Value linkning dataset has been saved to {output_file_path}\")\n",
    "\n",
    "output_file_path_list = \"assets/value_linking_dataset_list_spider_test.json\"\n",
    "processor.format_value_strings(output_file_path, output_file_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e89ea05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value linkning dataset has been saved to assets/value_linking_dataset_bird_train.json\n"
     ]
    }
   ],
   "source": [
    "with open(\"assets/train/train.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"assets/train/train_tables.json\", \"r\") as f:\n",
    "    schema_data = json.load(f)\n",
    "# Initialize the SQLQueryProcessor with schema data\n",
    "processor = ValueLinkingDatasetProcessor(schema_data)\n",
    "\n",
    "\n",
    "results = []\n",
    "for query in train_data:\n",
    "    sql_query = query[\"SQL\"]\n",
    "    db_id = query[\"db_id\"]\n",
    "    # Extract tables, columns, and values for the SQL query\n",
    "    tables, columns, values = processor.extract_tables_columns_and_values(sql_query, db_id)\n",
    "    results.append({\n",
    "        \"question\": query[\"question\"],\n",
    "        \"SQL\": sql_query,\n",
    "        \"tables\": tables,\n",
    "        \"columns\": columns,\n",
    "        \"values\": values,\n",
    "        \"db_id\": db_id,\n",
    "        \"source\": \"bird_train\",\n",
    "    })\n",
    "\n",
    "# Save results to a JSON file\n",
    "output_file_path = \"assets/value_linking_dataset_bird_train.json\"\n",
    "with open(output_file_path, \"w\") as outfile:\n",
    "    json.dump(results, outfile, indent=4)\n",
    "    \n",
    "print(f\"Value linkning dataset has been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bcadc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in assets/value_linking_dataset_bird_dev_numeric.json: 572\n",
      "Number of records in assets/value_linking_dataset_spider_dev_numeric.json: 107\n",
      "Number of records in assets/value_linking_dataset_spider_test_numeric.json: 250\n",
      "Number of records in assets/value_linking_valid_values_numeric.json: 929\n",
      "Number of records in assets/value_linking_valid_values_list_numeric.json: 929\n",
      "Number of records in assets/value_linking_valid_values_no_bird_train_numeric.json: 929\n"
     ]
    }
   ],
   "source": [
    "from create_value_linking_dataset import ValueLinkingDatasetProcessor\n",
    "import json\n",
    "input_paths = [\n",
    "    \"assets/value_linking_dataset_bird_dev.json\",\n",
    "    \"assets/value_linking_dataset_spider_dev.json\",\n",
    "    \"assets/value_linking_dataset_spider_test.json\",\n",
    "]\n",
    "output_paths = [\n",
    "    \"assets/value_linking_dataset_bird_dev_numeric.json\",\n",
    "    \"assets/value_linking_dataset_spider_dev_numeric.json\",\n",
    "    \"assets/value_linking_dataset_spider_test_numeric.json\",\n",
    "]\n",
    "\n",
    "for input_path, output_path in zip(input_paths, output_paths):\n",
    "    ValueLinkingDatasetProcessor.filter_json_file_for_digit_only(input_path, output_path)\n",
    "    #print the number of records in the output file\n",
    "    with open(output_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Number of records in {output_path}: {len(data)}\")\n",
    "    \n",
    "output_file_path = \"assets/value_linking_valid_values_numeric.json\"\n",
    "#merge the three output files and dump to output_file_path\n",
    "merged_data = []\n",
    "for output_path in output_paths:\n",
    "    with open(output_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        merged_data.extend(data)\n",
    "with open(output_file_path, \"w\") as outfile:\n",
    "    json.dump(merged_data, outfile, indent=4)\n",
    "output_file_path_list = \"assets/value_linking_valid_values_list_numeric.json\"\n",
    "processor.format_value_strings(output_file_path, output_file_path_list)\n",
    "\n",
    "with open(output_file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    print(f\"Number of records in {output_file_path}: {len(data)}\")\n",
    "\n",
    "with open(output_file_path_list, \"r\") as f:\n",
    "    data_list = json.load(f)\n",
    "    print(f\"Number of records in {output_file_path_list}: {len(data)}\")\n",
    "\n",
    "final_records = []\n",
    "for record,list_values in zip(data, data_list):\n",
    "    to_append = record\n",
    "    #add the list as a field to the record\n",
    "    to_append[\"values_list\"] = list_values\n",
    "    final_records.append(to_append)\n",
    "# Save results to a JSON file\n",
    "output_file_path = \"assets/value_linking_valid_values_no_bird_train_numeric.json\"\n",
    "with open(output_file_path, \"w\") as outfile:\n",
    "    json.dump(final_records, outfile, indent=4)\n",
    "\n",
    "#print the number of records in the output file\n",
    "with open(output_file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    print(f\"Number of records in {output_file_path}: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0415d941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in assets/value_linking_dataset_bird_dev_valid_values.json: 976\n",
      "Number of records in assets/value_linking_dataset_spider_dev_valid_values.json: 122\n",
      "Number of records in assets/value_linking_dataset_spider_test_valid_values.json: 229\n",
      "Number of records in assets/value_linking_valid_values.json: 1327\n",
      "Number of records in assets/value_linking_valid_values_list.json: 1327\n",
      "Number of records in assets/value_linking_valid_values_no_bird_train.json: 1327\n"
     ]
    }
   ],
   "source": [
    "input_paths = [\n",
    "    \"assets/value_linking_dataset_bird_dev.json\",\n",
    "    \"assets/value_linking_dataset_spider_dev.json\",\n",
    "    \"assets/value_linking_dataset_spider_test.json\",\n",
    "]\n",
    "output_paths = [\n",
    "    \"assets/value_linking_dataset_bird_dev_valid_values.json\",\n",
    "    \"assets/value_linking_dataset_spider_dev_valid_values.json\",\n",
    "    \"assets/value_linking_dataset_spider_test_valid_values.json\",\n",
    "]\n",
    "\n",
    "for input_path, output_path in zip(input_paths, output_paths):\n",
    "    processor.filter_json_file(input_path, output_path)\n",
    "    #print the number of records in the output file\n",
    "    with open(output_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Number of records in {output_path}: {len(data)}\")\n",
    "    \n",
    "output_file_path = \"assets/value_linking_valid_values.json\"\n",
    "#merge the three output files and dump to output_file_path\n",
    "merged_data = []\n",
    "for output_path in output_paths:\n",
    "    with open(output_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        merged_data.extend(data)\n",
    "with open(output_file_path, \"w\") as outfile:\n",
    "    json.dump(merged_data, outfile, indent=4)\n",
    "output_file_path_list = \"assets/value_linking_valid_values_list.json\"\n",
    "processor.format_value_strings(output_file_path, output_file_path_list)\n",
    "\n",
    "with open(output_file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    print(f\"Number of records in {output_file_path}: {len(data)}\")\n",
    "\n",
    "with open(output_file_path_list, \"r\") as f:\n",
    "    data_list = json.load(f)\n",
    "    print(f\"Number of records in {output_file_path_list}: {len(data)}\")\n",
    "\n",
    "final_records = []\n",
    "for record,list_values in zip(data, data_list):\n",
    "    to_append = record\n",
    "    #add the list as a field to the record\n",
    "    to_append[\"values_list\"] = list_values\n",
    "    final_records.append(to_append)\n",
    "# Save results to a JSON file\n",
    "output_file_path = \"assets/value_linking_valid_values_no_bird_train.json\"\n",
    "with open(output_file_path, \"w\") as outfile:\n",
    "    json.dump(final_records, outfile, indent=4)\n",
    "\n",
    "#print the number of records in the output file\n",
    "with open(output_file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    print(f\"Number of records in {output_file_path}: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c47f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in assets/value_linking_dataset_bird_dev_valid_values.json: 976\n",
      "Number of records in assets/value_linking_dataset_spider_dev_valid_values.json: 122\n",
      "Number of records in assets/value_linking_dataset_spider_test_valid_values.json: 229\n",
      "Number of records in assets/value_linking_dataset_bird_train_valid_values.json: 6286\n",
      "Number of records in assets/value_linking_valid_values.json: 7613\n",
      "Number of records in assets/value_linking_valid_values_list.json: 7613\n",
      "Number of records in assets/value_linking_valid_values.json: 7613\n"
     ]
    }
   ],
   "source": [
    "input_paths = [\n",
    "    \"assets/value_linking_dataset_bird_dev.json\",\n",
    "    \"assets/value_linking_dataset_spider_dev.json\",\n",
    "    \"assets/value_linking_dataset_spider_test.json\",\n",
    "    \"assets/value_linking_dataset_bird_train.json\"\n",
    "]\n",
    "output_paths = [\n",
    "    \"assets/value_linking_dataset_bird_dev_valid_values.json\",\n",
    "    \"assets/value_linking_dataset_spider_dev_valid_values.json\",\n",
    "    \"assets/value_linking_dataset_spider_test_valid_values.json\",\n",
    "    \"assets/value_linking_dataset_bird_train_valid_values.json\"\n",
    "]\n",
    "\n",
    "for input_path, output_path in zip(input_paths, output_paths):\n",
    "    processor.filter_json_file(input_path, output_path)\n",
    "    #print the number of records in the output file\n",
    "    with open(output_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Number of records in {output_path}: {len(data)}\")\n",
    "    \n",
    "output_file_path = \"assets/value_linking_valid_values.json\"\n",
    "#merge the three output files and dump to output_file_path\n",
    "merged_data = []\n",
    "for output_path in output_paths:\n",
    "    with open(output_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        merged_data.extend(data)\n",
    "with open(output_file_path, \"w\") as outfile:\n",
    "    json.dump(merged_data, outfile, indent=4)\n",
    "output_file_path_list = \"assets/value_linking_valid_values_list.json\"\n",
    "processor.format_value_strings(output_file_path, output_file_path_list)\n",
    "\n",
    "with open(output_file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    print(f\"Number of records in {output_file_path}: {len(data)}\")\n",
    "\n",
    "with open(output_file_path_list, \"r\") as f:\n",
    "    data_list = json.load(f)\n",
    "    print(f\"Number of records in {output_file_path_list}: {len(data)}\")\n",
    "\n",
    "final_records = []\n",
    "for record,list_values in zip(data, data_list):\n",
    "    to_append = record\n",
    "    #add the list as a field to the record\n",
    "    to_append[\"values_list\"] = list_values\n",
    "    final_records.append(to_append)\n",
    "# Save results to a JSON file\n",
    "output_file_path = \"assets/value_linking_valid_values.json\"\n",
    "with open(output_file_path, \"w\") as outfile:\n",
    "    json.dump(final_records, outfile, indent=4)\n",
    "\n",
    "#print the number of records in the output file\n",
    "with open(output_file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    print(f\"Number of records in {output_file_path}: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcb1a9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of records: 1006\n"
     ]
    }
   ],
   "source": [
    "from create_value_linking_dataset import ValueLinkingDatasetProcessor\n",
    "input_file=\"assets/value_linking_valid_values_no_bird_train.json\"\n",
    "output_file = \"assets/value_linking_valid_values_exact_no_bird_train.json\"\n",
    "\n",
    "ValueLinkingDatasetProcessor.filter_json_by_question_values(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a95008a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of records: 5547\n"
     ]
    }
   ],
   "source": [
    "from create_value_linking_dataset import ValueLinkingDatasetProcessor\n",
    "input_file=\"assets/value_linking_valid_values.json\"\n",
    "output_file = \"assets/value_linking_valid_values_exact.json\"\n",
    "\n",
    "ValueLinkingDatasetProcessor.filter_json_by_question_values(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1fc12a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of records: 1006\n"
     ]
    }
   ],
   "source": [
    "from create_value_linking_dataset import ValueLinkingDatasetProcessor\n",
    "\n",
    "input_file= \"assets/value_linking_valid_values_exact_no_bird_train.json\"\n",
    "output_file = \"assets/value_linking_valid_values_typos.json\"\n",
    "ValueLinkingDatasetProcessor.introduce_typos_in_question(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fd36d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_value_linking_dataset import ValueLinkingDatasetProcessor\n",
    "input_file=\"assets/value_linking_valid_values_exact.json\"\n",
    "output_file = \"assets/value_linking_valid_values_synonyms.json\"\n",
    "ValueLinkingDatasetProcessor.generate_synonyms_with_vllm_parsed(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83124adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_value_linking_dataset import ValueLinkingDatasetProcessor\n",
    "input_file= \"assets/value_linking_valid_values_exact_no_bird_train.json\"\n",
    "output_path = \"CHESS/data/value_linking/value_linking_valid_values_exact_no_bird_train.json\"\n",
    "ValueLinkingDatasetProcessor.prepare_data_chess(input_file, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb4e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_value_linking_dataset import ValueLinkingDatasetProcessor\n",
    "input_file= \"assets/value_linking_valid_values_exact_no_bird_train.json\"\n",
    "output_folder = \"/data/hdd1/users/akouk/value_linking/fresh_value_linking/experimental-analysis-of-value-inking/OmniSQL/value_linking/databases\"\n",
    "ValueLinkingDatasetProcessor.copy_databases(input_file, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67bd7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def merge_json_files(input_file_paths: list[str], output_file_path: str):\n",
    "    \"\"\"\n",
    "    Merges multiple JSON files (each containing a list of records) into a single\n",
    "    JSON file containing a list of all records.\n",
    "\n",
    "    Args:\n",
    "        input_file_paths: A list of paths to the input JSON files.\n",
    "        output_file_path: The path where the merged JSON file will be saved.\n",
    "    \"\"\"\n",
    "    all_records = []\n",
    "    for file_path in input_file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "            records = json.load(infile)\n",
    "            all_records.extend(records)\n",
    "\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(all_records, outfile, indent=4)\n",
    "        \n",
    "input_file_paths = [\n",
    "    \"assets/dev_20240627/dev_tables.json\",\n",
    "    \"assets/spider_data/tables.json\",\n",
    "    \"assets/spider_data/test_tables.json\",\n",
    "]\n",
    "\n",
    "output_file_path = \"assets/merged_tables.json\"\n",
    "merge_json_files(input_file_paths, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45936a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"assets/merged_tables.json\"\n",
    "input_folder = \"CHESS/data/value_linking/databses\"\n",
    "output_file = \"CHESS/data/value_linking/tables.json\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def filter_json_by_existing_db_ids(\n",
    "    merged_json_path: str,\n",
    "    databases_root_folder: str,\n",
    "    output_json_path: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Filters records in a JSON file based on whether their 'db_id' corresponds\n",
    "    to an existing subdirectory in the databases_root_folder.\n",
    "\n",
    "    Args:\n",
    "        merged_json_path: Path to the input JSON file (list of records).\n",
    "        databases_root_folder: Path to the folder containing database subdirectories\n",
    "                               (e.g., 'prepared_databases_output').\n",
    "        output_json_path: Path where the filtered JSON file will be saved.\n",
    "    \"\"\"\n",
    "    db_root = Path(databases_root_folder)\n",
    "    existing_db_ids = set()\n",
    "    for item in db_root.iterdir():\n",
    "        if item.is_dir():\n",
    "            existing_db_ids.add(item.name)\n",
    "\n",
    "    with open(merged_json_path, 'r', encoding='utf-8') as infile:\n",
    "        all_records = json.load(infile)\n",
    "\n",
    "    filtered_records = []\n",
    "    for record in all_records:\n",
    "        record_db_id = record.get(\"db_id\")\n",
    "        if record_db_id and record_db_id in existing_db_ids:\n",
    "            filtered_records.append(record)\n",
    "\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(filtered_records, outfile, indent=4)\n",
    "\n",
    "filter_json_by_existing_db_ids(\n",
    "    merged_json_path=input_file,\n",
    "    databases_root_folder=input_folder,\n",
    "    output_json_path=output_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b07dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-11 20:09:39,235] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_rwlock_trywrlock@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_getspecific@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_rwlock_timedrdlock@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_rwlock_rdlock@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /lib/x86_64-linux-gnu/librt.so.1: undefined reference to `__libc_unwind_link_get@GLIBC_PRIVATE'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `shm_open@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `dlvsym'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_rwlockattr_destroy@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_key_create@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `dlopen'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_key_delete@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_mutexattr_setpshared@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_setspecific@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_detach@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_rwlock_destroy@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_rwlock_wrlock@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_spin_init@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `shm_unlink@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_spin_unlock@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `sem_destroy@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_condattr_setpshared@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `sem_timedwait@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_once@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_spin_lock@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_mutexattr_init@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `sem_wait@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_rwlock_unlock@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_mutex_trylock@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `dlclose'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_mutexattr_destroy@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_join@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `sem_init@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_kill@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `dlerror'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `dlsym'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_spin_destroy@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_rwlockattr_init@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_rwlock_timedwrlock@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `sem_post@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libstdc++.so.6: undefined reference to `dladdr@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `clock_gettime@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_create@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_rwlock_init@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_rwlockattr_setpshared@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_rwlock_tryrdlock@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `pthread_mutexattr_settype@GLIBC_2.2.5'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /lib/x86_64-linux-gnu/libm.so.6: undefined reference to `__strtof128_nan@GLIBC_PRIVATE'\n",
      "/home/akouk/miniconda3/envs/tolis/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/akouk/miniconda3/envs/tolis/lib/libcufile.so: undefined reference to `sem_trywait@GLIBC_2.2.5'\n",
      "collect2: error: ld returned 1 exit status\n",
      "100%|██████████| 1006/1006 [00:10<00:00, 96.13it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from create_value_linking_dataset import ValueLinkingDatasetProcessor\n",
    "ValueLinkingDatasetProcessor.generate_prompts_for_eval_open_search(\"OpenSearch-SQL/value_linking/data_preprocess/dev.json\",\"OpenSearch-SQL/Bird/fewshot/questions.json\",\"OpenSearch-SQL/value_linking/fewshot/questions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82328a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2713\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('assets/value_linking_valid_values_exact.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "count_with_spaces = 0\n",
    "for item in data:\n",
    "    for val_obj in item[\"values\"]:\n",
    "        if \" \" in val_obj[\"value\"]:\n",
    "            count_with_spaces += 1\n",
    "print(count_with_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d439db63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "686\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "count_with_punctuation = 0\n",
    "punctuation_chars = set(string.punctuation)\n",
    "for item in data:\n",
    "    for val_obj in item[\"values\"]:\n",
    "        if any(char in punctuation_chars for char in val_obj[\"value\"]):\n",
    "            count_with_punctuation += 1\n",
    "print(count_with_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e778c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "count_with_keywords = 0\n",
    "keywords_to_check = [' and ', ' or ']\n",
    "\n",
    "for item in data:\n",
    "    for val_obj in item[\"values\"]:\n",
    "        value_str = val_obj[\"value\"]\n",
    "        if any(keyword in value_str for keyword in keywords_to_check):\n",
    "            count_with_keywords += 1\n",
    "print(count_with_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da68d09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'table': 'posts', 'column': 'title', 'value': 'Integration of Weka and/or RapidMiner into Informatica PowerCenter/Developer', 'condition': '='}\n",
      "{'table': 'users', 'column': 'websiteurl', 'value': 'http://stackoverflow.com', 'condition': '='}\n",
      "{'table': 'match', 'column': 'season', 'value': '2015/2016', 'condition': '='}\n",
      "{'table': 'match', 'column': 'season', 'value': '2010/2011', 'condition': '='}\n",
      "{'table': 'match', 'column': 'season', 'value': '2008/2009', 'condition': '='}\n",
      "{'table': 'lists_users', 'column': 'user_avatar_image_url', 'value': 'https://assets.mubicdn.net/images/avatars/74983/images-w150.jpg?1523895214', 'condition': '='}\n",
      "{'table': 'publishers', 'column': 'pub_name', 'value': 'Binnet & Hardley', 'condition': '='}\n",
      "{'table': 'repo', 'column': 'url', 'value': 'https://github.com/wallerdev/htmlsharp.git', 'condition': '='}\n",
      "{'table': 'repo', 'column': 'url', 'value': 'https://github.com/zphingphong/DiscardCustomerApp.git', 'condition': '='}\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "count_with_keywords = 0\n",
    "keywords_to_check = ['& ', '/' ,'|']\n",
    "\n",
    "for item in data:\n",
    "    for val_obj in item[\"values\"]:\n",
    "        value_str = val_obj[\"value\"]\n",
    "        if any(keyword in value_str for keyword in keywords_to_check):\n",
    "            count_with_keywords += 1\n",
    "            if count_with_keywords < 10:\n",
    "                print(val_obj)\n",
    "print(count_with_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ff9a92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'table': 'frpm', 'column': 'educational option type', 'value': 'Continuation School', 'condition': '='}\n",
      "{'table': 'schools', 'column': 'statustype', 'value': 'Active', 'condition': '='}\n",
      "{'table': 'frpm', 'column': 'charter funding type', 'value': 'Directly funded', 'condition': '='}\n",
      "{'table': 'frpm', 'column': 'high grade', 'value': '12', 'condition': '='}\n",
      "{'table': 'frpm', 'column': 'low grade', 'value': '9', 'condition': '='}\n",
      "{'table': 'frpm', 'column': 'free meal count (k-12)', 'value': '500', 'condition': '>'}\n",
      "{'table': 'satscores', 'column': 'cname', 'value': 'Contra Costa', 'condition': '='}\n",
      "{'table': 'schools', 'column': 'fundingtype', 'value': 'Locally funded', 'condition': '='}\n",
      "{'table': 'schools', 'column': 'fundingtype', 'value': 'Locally funded', 'condition': '='}\n",
      "6085\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "count_replaceable_values = 0\n",
    "\n",
    "for item in data:\n",
    "    for val_obj in item.get(\"values\", []):\n",
    "        value_str = val_obj.get(\"value\", \"\")\n",
    "        if not value_str:\n",
    "            continue\n",
    "\n",
    "        original_value_marked_replaceable = False\n",
    "        words = value_str.split()\n",
    "        \n",
    "        for word in words:\n",
    "            word_lower = word.lower()\n",
    "            if not word_lower:\n",
    "                continue\n",
    "\n",
    "            found_synonym_for_this_word = False\n",
    "            for synset in wn.synsets(word_lower):\n",
    "                for lemma in synset.lemmas():\n",
    "                    synonym_form = lemma.name().replace('_', ' ').lower()\n",
    "                    if synonym_form != word_lower:\n",
    "                        found_synonym_for_this_word = True\n",
    "                        break\n",
    "                if found_synonym_for_this_word:\n",
    "                    break\n",
    "            \n",
    "            if found_synonym_for_this_word:\n",
    "                original_value_marked_replaceable = True\n",
    "                break \n",
    "        \n",
    "        if original_value_marked_replaceable:\n",
    "            count_replaceable_values += 1\n",
    "            if count_replaceable_values < 10:\n",
    "                print(val_obj)\n",
    "\n",
    "print(count_replaceable_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78d66583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'table': 'frpm', 'column': 'educational option type', 'value': 'Continuation School', 'condition': '='}\n",
      "{'table': 'schools', 'column': 'county', 'value': 'Colusa', 'condition': '='}\n",
      "{'table': 'schools', 'column': 'admlname1', 'value': 'Larson', 'condition': '='}\n",
      "{'table': 'schools', 'column': 'doctype', 'value': 'Community College District', 'condition': '='}\n",
      "{'table': 'district', 'column': 'a2', 'value': 'Litomerice', 'condition': '='}\n",
      "{'table': 'district', 'column': 'a2', 'value': 'Pisek', 'condition': '='}\n",
      "{'table': 'molecule', 'column': 'molecule_id', 'value': 'TR002'}\n",
      "{'table': 'connected', 'column': 'atom_id', 'value': 'TR000_1', 'condition': '='}\n",
      "{'table': 'connected', 'column': 'atom_id', 'value': 'TR000_1', 'condition': '='}\n",
      "893\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "count_companies = 0\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "for item in data:\n",
    "    for val_obj in item.get(\"values\", []):\n",
    "        value_str = val_obj.get(\"value\", \"\")\n",
    "        if not value_str:\n",
    "            continue\n",
    "\n",
    "        doc = nlp(value_str.strip())\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"ORG\" and ent.text.strip() == value_str.strip():\n",
    "                count_companies += 1\n",
    "                if count_companies < 10:\n",
    "                    print(val_obj)\n",
    "                break\n",
    "print(count_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64f5fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'table': 'foreign_data', 'column': 'language', 'value': 'Japanese', 'condition': '='}\n",
      "{'table': 'foreign_data', 'column': 'language', 'value': 'Japanese', 'condition': '='}\n",
      "{'table': 'set_translations', 'column': 'language', 'value': 'Japanese', 'condition': '='}\n",
      "{'table': 'set_translations', 'column': 'language', 'value': 'Japanese', 'condition': '='}\n",
      "{'table': 'sets', 'column': 'block', 'value': 'Masques'}\n",
      "{'table': 'set_translations', 'column': 'language', 'value': 'Japanese', 'condition': '='}\n",
      "{'table': 'posts', 'column': 'title', 'value': 'Eliciting priors from experts', 'condition': '='}\n",
      "{'table': 'posts', 'column': 'title', 'value': 'Examples for teaching: Correlation does not mean causation', 'condition': '='}\n",
      "{'table': 'posts', 'column': 'title', 'value': 'Eliciting priors from experts', 'condition': '='}\n",
      "{'table': 'users', 'column': 'displayname', 'value': 'DatEpicCoderGuyWhoPrograms', 'condition': '='}\n",
      "{'table': 'posts', 'column': 'title', 'value': 'Open source tools for visualizing multi-dimensional data?', 'condition': '='}\n",
      "{'table': 'posts', 'column': 'title', 'value': 'Detecting a given face in a database of facial images', 'condition': '='}\n",
      "{'table': 'posts', 'column': 'title', 'value': 'What is the best introductory Bayesian statistics textbook?', 'condition': '='}\n",
      "{'table': 'badges', 'column': 'name', 'value': 'outliers', 'condition': '='}\n",
      "{'table': 'posts', 'column': 'title', 'value': 'How to tell if something happened in a data set which monitors a value over time', 'condition': '='}\n",
      "{'table': 'posts', 'column': 'title', 'value': 'What are principal component scores?', 'condition': '='}\n",
      "{'table': 'posthistory', 'column': 'text', 'value': 'Computer Game Datasets', 'condition': '='}\n",
      "{'table': 'posts', 'column': 'title', 'value': 'Analysing wind data with R', 'condition': '='}\n",
      "{'table': 'tags', 'column': 'tagname', 'value': 'careers', 'condition': '='}\n",
      "{'table': 'posts', 'column': 'title', 'value': 'Clustering 1D data', 'condition': '='}\n",
      "{'table': 'posts', 'column': 'title', 'value': 'Group differences on a five point Likert item', 'condition': '='}\n",
      "{'table': 'publisher', 'column': 'publisher_name', 'value': 'Marvel Comics', 'condition': '='}\n",
      "{'table': 'publisher', 'column': 'publisher_name', 'value': 'Marvel Comics', 'condition': '='}\n",
      "{'table': 'publisher', 'column': 'publisher_name', 'value': 'Marvel Comics', 'condition': '='}\n",
      "{'table': 'publisher', 'column': 'publisher_name', 'value': 'Marvel Comics', 'condition': '='}\n",
      "{'table': 'publisher', 'column': 'publisher_name', 'value': 'Marvel Comics', 'condition': '='}\n",
      "{'table': 'publisher', 'column': 'publisher_name', 'value': 'Marvel Comics', 'condition': '='}\n",
      "{'table': 'publisher', 'column': 'publisher_name', 'value': 'Marvel Comics', 'condition': '='}\n",
      "{'table': 'publisher', 'column': 'publisher_name', 'value': 'Dark Horse Comics', 'condition': '='}\n",
      "{'table': 'publisher', 'column': 'publisher_name', 'value': 'Dark Horse Comics', 'condition': '='}\n",
      "{'table': 'publisher', 'column': 'publisher_name', 'value': 'Dark Horse Comics', 'condition': '='}\n",
      "{'table': 'publisher', 'column': 'publisher_name', 'value': 'Marvel Comics', 'condition': '='}\n",
      "{'table': 'publisher', 'column': 'publisher_name', 'value': 'Marvel Comics', 'condition': '='}\n",
      "{'table': 'publisher', 'column': 'publisher_name', 'value': 'Dark Horse Comics', 'condition': '='}\n",
      "{'table': 'publisher', 'column': 'publisher_name', 'value': 'Marvel Comics', 'condition': '='}\n",
      "{'table': 'circuits', 'column': 'name', 'value': 'Brands Hatch', 'condition': '='}\n",
      "{'table': 'drivers', 'column': 'nationality', 'value': 'Japanese', 'condition': '='}\n",
      "{'table': 'constructors', 'column': 'nationality', 'value': 'Japanese', 'condition': '='}\n",
      "{'table': 'drivers', 'column': 'nationality', 'value': 'Japanese', 'condition': '='}\n",
      "{'table': 'team', 'column': 'team_long_name', 'value': 'Queens Park Rangers', 'condition': '='}\n",
      "{'table': 'country', 'column': 'name', 'value': 'Netherlands', 'condition': '='}\n",
      "{'table': 'major', 'column': 'major_name', 'value': 'Law and Constitutional Studies', 'condition': '='}\n",
      "{'table': 'major', 'column': 'college', 'value': 'College of Humanities and Social Sciences', 'condition': '='}\n",
      "{'table': 'income', 'column': 'source', 'value': 'Dues', 'condition': '='}\n",
      "{'table': 'income', 'column': 'source', 'value': 'Dues', 'condition': '='}\n",
      "{'table': 'major', 'column': 'college', 'value': 'College of Humanities and Social Sciences', 'condition': '='}\n",
      "{'table': 'major', 'column': 'department', 'value': 'School of Applied Sciences, Technology and Education', 'condition': '='}\n",
      "{'table': 'major', 'column': 'major_name', 'value': 'Physics', 'condition': '='}\n",
      "{'table': 'expense', 'column': 'expense_description', 'value': 'Posters', 'condition': '='}\n",
      "{'table': 'major', 'column': 'department', 'value': 'School of Applied Sciences, Technology and Education', 'condition': '='}\n",
      "633\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "with open('assets/value_linking_valid_values_exact.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "count_plural_values = 0\n",
    "\n",
    "for item in data:\n",
    "    for val_obj in item.get(\"values\", []):\n",
    "        value_str = val_obj.get(\"value\", \"\")\n",
    "        if not value_str:\n",
    "            continue\n",
    "\n",
    "        doc = nlp(value_str)\n",
    "        value_is_plural = False\n",
    "        for token in doc:\n",
    "            if token.tag_ in [\"NNS\", \"NNPS\"]:\n",
    "                value_is_plural = True\n",
    "                break\n",
    "        \n",
    "        if value_is_plural:\n",
    "            if count_plural_values < 50:\n",
    "                print(val_obj)\n",
    "            count_plural_values += 1\n",
    "\n",
    "print(count_plural_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc0f3aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import regex as re\n",
    "\n",
    "with open('assets/value_linking_valid_values_exact.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "scientific_abbreviations_set = {\n",
    "    \"%\",\n",
    "}\n",
    "\n",
    "unit_pattern = '|'.join(sorted(map(re.escape, scientific_abbreviations_set), key=len, reverse=True))\n",
    "pattern = re.compile(\n",
    "    rf'(?<=^|\\s)(?:\\d+(\\.\\d+)?){0,1}'\n",
    "    rf'(?P<unit>{unit_pattern})(?=(?:\\s|\\.|$))'\n",
    ")\n",
    "\n",
    "count_scientific_abbr = 0\n",
    "\n",
    "for item in data:\n",
    "    for val_obj in item.get(\"values\", []):\n",
    "        value_str = val_obj.get(\"value\", \"\")\n",
    "        if not value_str:\n",
    "            continue\n",
    "        if pattern.search(value_str):\n",
    "            count_scientific_abbr += 1\n",
    "\n",
    "print(count_scientific_abbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f2e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sqlite3\n",
    "\n",
    "def print_sample_rows(sqlite_path, num_rows=5):\n",
    "    \"\"\"\n",
    "    Connects to a SQLite database, discovers all tables, and prints\n",
    "    up to num_rows sample rows from each table.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Database: {sqlite_path} ===\")\n",
    "    conn = sqlite3.connect(sqlite_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Get all table names\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "    if not tables:\n",
    "        print(\"No tables found.\")\n",
    "    for table in tables:\n",
    "        print(f\"\\n-- Table: {table} --\")\n",
    "        try:\n",
    "            cursor.execute(f\"SELECT * FROM `{table}` LIMIT {num_rows};\")\n",
    "            rows = cursor.fetchall()\n",
    "            # Print header (column names)\n",
    "            col_names = [d[0] for d in cursor.description]\n",
    "            print(\" | \".join(col_names))\n",
    "            print(\"-\" * (len(col_names) * 15))\n",
    "            # Print rows\n",
    "            for r in rows:\n",
    "                print(\" | \".join(str(x) for x in r))\n",
    "            if not rows:\n",
    "                print(\"(no rows)\")\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"Error querying table {table}: {e}\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "def main(root_dir, pattern=\"*.sqlite\"):\n",
    "    \"\"\"\n",
    "    Walk through root_dir, find all .sqlite files matching pattern,\n",
    "    and print sample rows for each.\n",
    "    \"\"\"\n",
    "    # Recursively find all .sqlite files\n",
    "    search_pattern = os.path.join(root_dir, \"**\", pattern)\n",
    "    sqlite_files = glob.glob(search_pattern, recursive=True)\n",
    "\n",
    "    if not sqlite_files:\n",
    "        print(\"No SQLite files found.\")\n",
    "        return\n",
    "\n",
    "    for sqlite_path in sqlite_files:\n",
    "        print_sample_rows(sqlite_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Adjust this path to the parent directory containing all {db_id} folders\n",
    "    bird_root = \"assets/dev_20240627/dev_databases\"\n",
    "    main(bird_root)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0be79e",
   "metadata": {},
   "source": [
    "Deduplciate the benchmark after verification for CHESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2304d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_value_linking_dataset import ValueLinkingDatasetProcessor\n",
    "input_file = \"assets/all_benchmarks/all_benchmarks_combined_verified.json\"\n",
    "output_file = \"CHESS/data/value_linking/all_benchmarks_combined_verified.json\"\n",
    "\n",
    "ValueLinkingDatasetProcessor.benchmark_deduplicate_chess(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1802756a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"punctuation_removal\": 73,\n",
      "    \"abbreviation_substitution\": 431,\n",
      "    \"word_reorder\": 192,\n",
      "    \"antonym_negation_rewrite\": 12,\n",
      "    \"full_question_paraphrase\": 371,\n",
      "    \"punctuation_alteration\": 73,\n",
      "    \"middle_space_addition\": 839,\n",
      "    \"transpose_char_typo\": 940,\n",
      "    \"word_addition_enrichment\": 746,\n",
      "    \"substitute_char_typo\": 884,\n",
      "    \"selective_space_removal\": 348,\n",
      "    \"delete_char_typo\": 869,\n",
      "    \"insert_char_typo\": 961,\n",
      "    \"word_removal_shortening\": 19,\n",
      "    \"singular_plural_toggle\": 40\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from create_value_linking_dataset import ValueLinkingDatasetProcessor\n",
    "input_file = \"assets/all_benchmarks/all_benchmarks_combined_verified.json\"\n",
    "\n",
    "returned_dict = ValueLinkingDatasetProcessor.count_alteration_types(input_file)\n",
    "#print the dict in a nice way\n",
    "import json\n",
    "print(json.dumps(returned_dict, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tolis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
