import os
import time
import yaml
from loguru import logger
from tqdm import tqdm
import pandas as pd
import string
import itertools
from sqlglot import parse_one

from pydantic import BaseModel, ConfigDict, Field

from loguru import logger
from tqdm import tqdm
import json
from typing import Any, Callable, List, Optional, Tuple, Dict, Union, Literal
import glob

from dotenv import load_dotenv
from huggingface_hub import snapshot_download
import gzip
import shutil
import zipfile
from utils.sqlite_db import DatabaseSqlite
from collections import defaultdict
import sqlite3
from torchmetrics import Metric
import random
from collections import Counter
import hashlib
from pydantic import BaseModel, field_validator, model_validator
from typing_extensions import Self
import requests
import re
import logging

OLLAMA_BASE_URL = "http://gaia-gpu-2.imsi.athenarc.gr:11434"

"""Templates for Text-to-SQL prompts."""

INSTRUCTIONS_TEMPLATE = """\
### Your task is to convert a question into a SQL query, given a {database_type} database schema.
Adhere to these rules:
- **Deliberately go through the question and database schema word by word** to appropriately answer the question
- **Use Table Aliases** to prevent ambiguity. For example, `SELECT table1.col1, table2.col1 FROM table1 JOIN table2 ON table1.id = table2.id`.
- When creating a ratio, always cast the numerator as float
Only the SQL query will be generated without further explanation.

This query will run on a database whose schema is represented in this string:

{database_schema}
"""

QUESTION_TEMPLATE = """\
### Input:
Generate a SQL query that answers the question `{nl_query}`.
"""

VALUE_LINKING_TEMPLATE = """\
### Retrieved Values:
The following values have been retrieved from the database and might be relevant to your query. Each value follows the format 'table.column.value':

{value_links}
Consider these values when constructing your query:
- Use these values to understand the format and type of data in each column
- Include relevant values in WHERE clauses if they match the question intent
- Be aware that not all retrieved values may be relevant to the question
"""

PROMPT_ANSWER_TEMPLATE = """\
### Response:
Based on your instructions, here is the SQL query I have generated to answer the question `{nl_query}`:
"""

CHAT_SQL_TEMPLATE = """\
```sql
{sql_query}
```
"""


class SqlQueryDatapoint(BaseModel):
    """
    The common representation of a Text2SQL / SQL2Text datapoint.
    New **optional** fields can be added freely but deleting/renaming existing fields
    will result to compatibility issues.

    **Example**
    ```python
    datapoint = SqlQueryDatapoint(
                nl-query="List all singers",
                sql_query="select * from singer",
                db_id: "singer",
                db_path: "databases/singer.sqlite"
            )
    ```

    **Arguments**

    * `nl_query (str)`: A natural language question.
    * `sql_query (str)`: A SQL query.
    * `db_id (str)`: The id of the database upon which the question is made.
    * `db_path (str)`: The path to the database.
    * `db_schema (Dict[str, Union[str, List]])`: The schema of the database.
        The dictionary must contain the following fields:

        - `table_names (list[<table_name>])`: The tables of the database.
        - `column_names (list[tuple[<table_idx>, <column_name>]])`: The columns of the schema.
        - `foreign_keys (list[tuple[<foreign_key_column_idx>, <foreign_key_column_idx>]])`: The list with the
            foreign primary key relations in the database.
    """

    model_config = ConfigDict(arbitrary_types_allowed=True, extra="allow")
    model_config["protected_namespaces"] = ()

    nl_query: str
    sql_query: str
    db_id: str
    db_path: str
    db_schema: List[Dict] = Field(default=None)

    # The pre-processed input as it is given to the model
    model_input: Optional[Union[str, List]] = None
    # The raw output generated by the model, before any processing
    model_output: Optional[str] = None
    # The raw output that the model should generate, maybe used to calculate loss
    expected_output: Optional[str] = None
    # Result of post-processing on the model's raw output
    prediction: Optional[str] = None
    # Used to calculate metrics
    ground_truth: Optional[str] = None


def get_value_links(datapoint: SqlQueryDatapoint,data) -> List[str]:
    query = datapoint.nl_query
    return data[query]

PROMPT_TEMPLATE = (
    INSTRUCTIONS_TEMPLATE + "\n\n" + QUESTION_TEMPLATE + "\n\n" + VALUE_LINKING_TEMPLATE + "\n\n" + PROMPT_ANSWER_TEMPLATE
)

PROMPT_TEMPLATE_NO_VALUE = (
    INSTRUCTIONS_TEMPLATE + "\n\n" + QUESTION_TEMPLATE + "\n\n" + PROMPT_ANSWER_TEMPLATE
)
VALUE_LEN_THRESHOLD = 30
def get_value_sequence(values: List) -> str:
    value_sequence = "values : "
    for i, value in enumerate(values):
        # Shorten very long values
        if type(value) == str and len(value) > VALUE_LEN_THRESHOLD:
            value = value[:VALUE_LEN_THRESHOLD] + "..."

        value_sequence += f"{value.__repr__()}"

        if i + 1 != len(values):
            # Do not add a comma after the last value
            value_sequence += " , "

    return value_sequence
def get_compact_schema_sequence(
    schema, include_pk, include_fk, include_notes, values_num, categorical_threshold
):
    """Create the serialized string sequence of the DB schema.

    Args:
        include_fk (bool): Whether or not to include foreign key information.
    """

    foreign_keys = []
    schema_sequence = ""

    for table in schema:
        table_name = table["table_name"]

        # Get table note if there is one
        if include_notes and table.get("note", None) is not None:
            table_note = " ( comment : " + table["note"] + " )"
        else:
            table_note = ""

        column_info_list = []
        for column in table["columns"]:
            # Get column name
            column_name = column["column"]

            additional_column_info = []

            # Get column type
            additional_column_info.append(column["data_type"])

            # Get PK indicator
            if column["is_pk"] and include_pk:
                additional_column_info.append("primary key")

            # Get foreign keys
            if len(column["foreign_keys"]) > 0:
                for fk in column["foreign_keys"]:
                    foreign_keys.append(
                        (
                            table_name,
                            column_name,
                            fk["foreign_table"],
                            fk["foreign_column"],
                        )
                    )

            # Get column note
            if include_notes and column.get("note", None) is not None:
                additional_column_info.append("comment : " + column["note"])

            # Get sample values
            if values_num != 0 and len(column["values"]) != 0:
                value_sequence = get_value_sequence(
                    column["values"][:values_num]
                    if len(column["values"]) > categorical_threshold
                    else column["values"]
                )
                additional_column_info.append(value_sequence)

            column_info_list.append(
                table_name
                + "."
                + column_name
                + " ( "
                + " | ".join(additional_column_info)
                + " )"
            )

        schema_sequence += (
            "table "
            + table_name
            + table_note
            + " , columns = [ "
            + " , ".join(column_info_list)
            + " ]\n"
        )

    if include_fk and len(foreign_keys) != 0:
        schema_sequence += "foreign keys :\n"
        for foreign_key in foreign_keys:
            schema_sequence += "{}.{} = {}.{}\n".format(
                foreign_key[0], foreign_key[1], foreign_key[2], foreign_key[3]
            )

    return schema_sequence.strip()



def get_db_schema_sequence(
    schema: List,
    type: Literal["ddl", "compact", "m-schema"] = "ddl",
    include_pk: bool = True,
    include_fk: bool = True,
    include_notes: bool = True,
    values_num: int = 0,
    categorical_threshold: int = None,
    db_id: str = "Anonymous",
) -> str:

    return get_compact_schema_sequence(
        schema,
        include_pk,
        include_fk,
        include_notes,
        values_num,
        categorical_threshold,
    )
class ZeroShotPromptFactory():
    name: str = "zero_shot_prompt"

    def __init__(
        self,
        schema_type: Literal["ddl", "compact"] = "ddl",
        include_pk: bool = True,
        include_fk: bool = True,
        include_notes: bool = True,
        values_num: int = 0,
        categorical_threshold: int = None,
        value_linking_path: Optional[str] = None
    ) -> None:
        self.schema_type = schema_type
        self.include_pk = include_pk
        self.include_fk = include_fk
        self.include_notes = include_notes
        self.values_num = values_num
        self.categorical_threshold = categorical_threshold
        self.value_linking_path = value_linking_path 

    def prompt_template(self):
        return PROMPT_TEMPLATE
    
    def prompt_template_no_value(self):
        return PROMPT_TEMPLATE_NO_VALUE
    def _get_db_schema_sequence(self, datapoint: SqlQueryDatapoint) -> str:
        """Creates the DB schema sequence for the given datapoint."""
        return get_db_schema_sequence(
            datapoint.db_schema,
            type=self.schema_type,
            include_pk=self.include_pk,
            include_fk=self.include_fk,
            include_notes=self.include_notes,
            values_num=self.values_num,
            categorical_threshold=self.categorical_threshold,
            db_id=datapoint.db_id
        )

    def _get_db_type(self, datapoint: SqlQueryDatapoint) -> str:
        # TODO: Is this the best way to get the DB type?
        return "SQLite" if datapoint.db_path.endswith(".sqlite") else "Postgres"
    
    def fill_prompt(self, datapoint: SqlQueryDatapoint) -> str:
        """Fills the prompt string template for the given datapoint.

        Args:
            datapoint (SqlQueryDatapoint): The datapoint that will be used as a
                question for the LLM.

        Returns:
            str: The prompt for the LLM as a string.
        """
        schema_sequence = self._get_db_schema_sequence(datapoint)
        database_type = self._get_db_type(datapoint)
        if self.value_linking_path:
            with open(self.value_linking_path, 'r') as file:
                data = json.load(file)
            transformed_data = {item['question']: item['values'] for item in data}
            value_links = get_value_links(datapoint,transformed_data)
            return self.prompt_template().format(
                nl_query=datapoint.nl_query,
                database_type=database_type,
                database_schema=schema_sequence,
                value_links=value_links,
            )
        else:
            return self.prompt_template_no_value().format(
                nl_query=datapoint.nl_query,
                database_type=database_type,
                database_schema=schema_sequence,
            )

    def extract_sql(self, response: str) -> str:
        """Extract sql from a markdown fenced code block.

        The function will return everything inside a fenced code block, taking
        into account that there might be a "sql" language specification next to
        the opening backticks.
        If the extraction fails, the function returns the given response.
        """
        # NOTE: The *? symbol matches as little text as possible
        pattern_recipe = r"```(sql)?([\S\s]*?)```"
        match = re.search(pattern_recipe, response, flags=re.IGNORECASE)

        if match is None:
            return response
        match_text = match.group(2)

        return match_text

def ollama_text_to_sql(
    datapoint: SqlQueryDatapoint,
    config: dict,
    prompt_factory: ZeroShotPromptFactory,
    examples: Optional[List[SqlQueryDatapoint]] = None,
) -> SqlQueryDatapoint:
    """Uses ollama to perform Text-to-SQL."""


    prompt = prompt_factory.fill_prompt(datapoint)
    datapoint.model_input = prompt
    generated_text = generate(prompt, config)

    datapoint.model_output = generated_text
    # Get the sql from the response
    if generated_text is not None:
        sql = prompt_factory.extract_sql(generated_text)
    else:
        sql = None
    datapoint.prediction = sql

    return datapoint


def generate(prompt: str, config: Dict) -> Optional[str]:
    """Generate a response to the given prompt with ollama.

    Args:
        prompt (str): The input prompt for the LLM.
        config (Dict): The configuration that contains:
            - name: The name of the model used and
            - options (Optional): A dictionary with extra options for the model.

    Returns:
        If the operation was successful the generated text is returned, otherwise None.
    """
    request_body = {
        "model": config["name"],
        "prompt": prompt,
        "stream": False,
    }

    # Add extra options (e.g., temperature)
    if "options" in config:
        request_body.update({"options": config["options"]})

    try:
        response = requests.post(
            f"{OLLAMA_BASE_URL}/api/generate", 
            data=json.dumps(request_body), 
            timeout=300  # 5-minute timeout
        )
        response.raise_for_status()
        generated_text = response.json().get("response")  # Avoid KeyError with .get()
    except requests.exceptions.HTTPError as e:
        logger.error(f"HTTP error: {e}")
        generated_text = None
    except requests.exceptions.Timeout:
        logger.error("Request timed out after 5 minutes")
        generated_text = None
    except requests.exceptions.RequestException as e:
        logger.error(f"Request error: {e}")
        generated_text = None
    except ValueError as e:  # Handle invalid JSON
        logger.error(f"JSON decode error: {e}")
        generated_text = None

    return generated_text

def handle_text_to_sql(
    datapoint: SqlQueryDatapoint,
    config: Dict,
) -> str:
    """
    This is a temporary function to unify different bases that perform Text-to-SQL.

    Args:
        datapoint: The datapoint that contains the natural language question that will be translated and information
                    about the queried database.
        config: Dictionary of configuration parameters. The dictionary must contain:
            - prompt: A dictionary with the prompt information that will be used as input to the model.
            - model: Information about the model used for the translation.
        example_retriever: The class responsible for retrieving similar examples.

    Returns:
        The translated SQL query.
    """
    # Get prompt factory
    db_representation_config = config["prompt"]["database_representation"]

    prompt_arguments = {
        "prompt_version": config["prompt"]["version"],
        "schema_type": db_representation_config["format"],
        "include_pk": db_representation_config["include_primary_keys"],
        "include_fk": db_representation_config["include_foreign_keys"],
        "include_notes": db_representation_config["include_notes"],
        "values_num": db_representation_config["example_values"]["num"],
        "categorical_threshold": db_representation_config["example_values"][
            "categorical_threshold"
        ],
        "value_linking_path": db_representation_config.get("value_linking_path"),
    }
    prompt_factory = ZeroShotPromptFactory(**prompt_arguments)

    examples = None

    # Generate SQL query with Ollama
    datapoint = ollama_text_to_sql(
        datapoint,
        config["model"],
        prompt_factory,
        examples=examples,
    )

    return datapoint.prediction

class ExperimentConfigModel(BaseModel):
    name: str  # Every model available in the ollama API
    options: Optional[
        dict
    ]  # In the options can be added all the options available in the ollama API
    mode: Literal["generate", "chat"]



class ExperimentDbRepresentationExampleValues(BaseModel):
    num: int
    categorical_threshold: Optional[int] = None

    @model_validator(mode="after")
    def categorical_threshold_check(self) -> Self:
        if self.categorical_threshold is None and self.num != 0:
            raise ValueError(
                "The categorical threshold must be given when the number of sample examples is not zero!"
            )
        elif (
            self.categorical_threshold is not None
            and self.categorical_threshold < self.num
        ):
            logger.warning(
                "The categorical threshold for the values of a column has been set to "
                "less that the number of example values"
            )
        return self

    @field_validator("num")
    @classmethod
    def num_check(cls, num: int) -> int:
        if num < 0:
            raise ValueError(
                "The number of column examples must be greater of equal to zero!"
            )
        return num


class ExperimentConfigDbRepresentation(BaseModel):
    format: Literal["ddl", "compact"]  # The format of the database schema.
    example_values: ExperimentDbRepresentationExampleValues
    include_primary_keys: (
        bool  # If True, the schema will contain information about the primary keys.
    )
    include_foreign_keys: (
        bool  # If True, the schema will contain information about the foreign keys.
    )
    include_notes: bool  # If True, the schema will contain notes about the schema elements (e.g., column descriptions).
    value_linking_path: Optional[str] = None

class ExperimentConfigPrompt(BaseModel):

    version: str  
    database_representation: ExperimentConfigDbRepresentation



class ExperimentConfig(BaseModel):
    """
    The configuration of a text-to-SQL experiment.
    """

    model: ExperimentConfigModel
    prompt: ExperimentConfigPrompt
    evaluated_dataset: str  # The dataset that will be used to run the experiment.
    download_dir: Optional[str] = None

MAPPING_FILE = "mapping.json"
CACHE_DIR = "assets/query_results_cache/"

class QueryCache:
    def __init__(self, cache_dir: str = CACHE_DIR):
        if not os.path.exists(cache_dir):
            logger.info(f"Cache directory does not exist. Creating at {cache_dir}")
            os.makedirs(cache_dir)

        if not os.path.exists(cache_dir + MAPPING_FILE):
            with open(cache_dir + MAPPING_FILE, "w") as f:
                f.write("{}")

        self.cache_dir = cache_dir

        with open(cache_dir + MAPPING_FILE, "r") as f:
            self.mapping: Dict[str, str] = json.load(f)

    @staticmethod
    def hash_query_value(query: str) -> str:
        hash_object = hashlib.sha256()
        hash_object.update(query.encode("utf-8"))
        return hash_object.hexdigest()

    def exists(self, query: str) -> bool:
        return self.hash_query_value(query) in self.mapping

    def get(self, query: str) -> Optional[pd.DataFrame]:
        if not self.exists(query):
            return None
        results_path = self.mapping[self.hash_query_value(query)]
        return pd.read_csv(results_path)

    def set(self, query: str, results: pd.DataFrame) -> None:
        results_path = self.cache_dir + self.hash_query_value(query) + ".csv"
        results.to_csv(results_path, index=False)
        self.mapping[self.hash_query_value(query)] = results_path
        with open(self.cache_dir + MAPPING_FILE, "w") as f:
            json.dump(self.mapping, f)

def _map_results(
    result1: pd.DataFrame, result2: pd.DataFrame, mappings: dict
) -> (pd.DataFrame, pd.DataFrame):
    # Remove columns that are not mapped
    mappings = {k: v for k, v in mappings.items() if v is not None}

    result1 = result1[list(mappings.keys())]
    result1 = result1.rename(columns=mappings)

    if len(result2.columns) != len(result1.columns):
        result2 = result2[list(mappings.values())]

    return result1, result2


def _valid_column_mapping(comb: dict) -> bool:
    # If the combination contains only one column and it is None or the combination contain only 1 value
    if (len(comb) == 1 and list(comb.values())[0] is None) or set(comb.values()) == {
        None
    }:
        return False

    # Check that each column is used at most once in the mapping values
    value_count = Counter(list(comb.values()))
    value_count.pop(None, None)

    if sum(value_count.values()) != len(value_count.values()):
        return False

    return True

def _constrain_mappings(
    mappings: dict, row: list, results: Union[pd.DataFrame, list]
) -> None:
    """
    Reduce the given mappings based on the constraints in order the given row to be equal with a result row.
    Args:
        mappings: A dictionary with the keys the row columns' indexes and values the results columns' indexes.
        row: A list of column values
        results: A dataframe or a list with which the row is compared.
    """
    row_column_num = len(row)

    if isinstance(results, pd.DataFrame):
        for i in range(row_column_num):
            i_mappings = mappings[i].copy()
            for j in i_mappings:
                if row[i] not in results[j].values:
                    mappings[i].remove(j)
    else:
        for i in range(row_column_num):
            i_mappings = mappings[i].copy()
            for j in i_mappings:
                if type(row[i]) is not type(results[j]) or row[i] != results[j]:
                    mappings[i].remove(j)

def _valid_final_mapping(possible_mappings: dict) -> bool:
    """
    Checks if a mapping dictionary is valid. A mapping is considered valid if all keys are mapped to 1 or 0 values and
    no value exists more than one times.
    """
    if not all([len(possible_mappings[i]) <= 1 for i in range(len(possible_mappings))]):
        return False

    # Check that there are no duplicates except for None
    values_count = Counter(
        list([value[0] if len(value) else None for value in possible_mappings.values()])
    )
    if None in values_count.keys():
        values_count.pop(None)

    if any(value_count > 1 for value_count in values_count.values()):
        return False

    return True
def _valid_column_mapping(comb: dict) -> bool:
    # If the combination contains only one column and it is None or the combination contain only 1 value
    if (len(comb) == 1 and list(comb.values())[0] is None) or set(comb.values()) == {
        None
    }:
        return False

    # Check that each column is used at most once in the mapping values
    value_count = Counter(list(comb.values()))
    value_count.pop(None, None)

    if sum(value_count.values()) != len(value_count.values()):
        return False

def _get_valid_column_mappings(column_possible_mappings: dict) -> list[dict]:
    """Returns the column mapping combinations based on which 2 dataframes can be compared"""

    # Add None as possible mapping value to all values
    column_possible_mappings = {
        k: v + [None] for k, v in column_possible_mappings.items()
    }

    # Get all permutations
    columns_combinations = (
        list(itertools.product(*column_possible_mappings.values()))
        if len(column_possible_mappings) > 1
        else list(column_possible_mappings.values())
    )
    column_mapping_combs = [
        {
            k: v
            for k, v in zip(
                list(column_possible_mappings.keys()), list(columns_combination)
            )
        }
        for columns_combination in columns_combinations
    ]

    # Remove invalid combinations
    column_mapping_combs = [
        column_valid_comb
        for column_valid_comb in column_mapping_combs
        if _valid_column_mapping(column_valid_comb)
    ]

    # Order combinations with descending number of None values
    sorted_column_mapping_combs = sorted(
        column_mapping_combs, key=lambda x: Counter(x.values())[None]
    )


def _constrain_columns_mappings(
    result1: pd.DataFrame, result2: pd.DataFrame, order_matters: bool
) -> dict:
    """
    Creates a dictionary with the mapping of columns from result1 with result2.
    """

    # Create all possible mappings
    possible_mappings = {
        i: [j for j in range(result2.shape[1])] for i in range(result1.shape[1])
    }

    iterations = 0
    while not _valid_final_mapping(possible_mappings) and iterations < 10:
        stable_row_idx = random.randint(0, result1.shape[0] - 1)

        if order_matters:
            _constrain_mappings(
                possible_mappings,
                result1.iloc[stable_row_idx].tolist(),
                result2.iloc[stable_row_idx],
            )

        else:
            _constrain_mappings(
                possible_mappings, result1.iloc[stable_row_idx].tolist(), result2
            )

        iterations += 1

    return possible_mappings

def _results_comparison(
    result1: pd.DataFrame, result2: pd.DataFrame, order_matters: bool
) -> Literal[
    "equal", "columns_subset", "columns_superset", "columns_intersect", "different"
]:
    if result1.shape[0] == 0 and result2.shape[0] == 0:
        return "equal"

    if result1.shape[0] != result2.shape[0]:
        return "different"

    # Remove column names
    result1.columns = [i for i in range(result1.shape[1])]
    result2.columns = [i for i in range(result2.shape[1])]

    # Compare results
    if result1.equals(result2):
        return "equal"

    columns_num_difference = len(result1.columns) - len(result2.columns)

    if columns_num_difference > 0:
        columns_mappings = _constrain_columns_mappings(result1, result2, order_matters)
        reverse = False
    else:
        columns_mappings = _constrain_columns_mappings(result2, result1, order_matters)
        reverse = True

    # For every valid mapping between the columns of the two results
    for column_mapping_comb in _get_valid_column_mappings(columns_mappings):
        # Map results
        if not reverse:
            result1_edited, result2_edited = _map_results(
                result1, result2, column_mapping_comb
            )
        else:
            result2_edited, result1_edited = _map_results(
                result2, result1, column_mapping_comb
            )

        # Order the results to compare them
        if not order_matters:
            columns = list(result1_edited.columns)
            result1_edited = result1_edited.sort_values(by=columns).reset_index(
                drop=True
            )
            result2_edited = result2_edited.sort_values(by=columns).reset_index(
                drop=True
            )

        none_mappings_num = list(column_mapping_comb.values()).count(None)

        if result1_edited.equals(result2_edited):
            if none_mappings_num > 0:
                if columns_num_difference != 0:
                    return "columns_superset" if not reverse else "columns_subset"
                else:
                    return "columns_intersect"
            else:
                return "equal"

    return "different"
def cache_query_results(query_execution_func):
    def wrapper(*args, **kwargs):
        if "sql" not in kwargs:
            logger.error(
                'To enable caching, the "sql" parameter must be passed as a keyword argument.'
            )
        query = kwargs["sql"]

        cache = QueryCache(CACHE_DIR)
        if cache.exists(query):
            logger.info("Query results found in cache. Returning cached results.")
            return cache.get(query)

        results = query_execution_func(*args, **kwargs)

        if not isinstance(results, Dict):
            # Avoid caching error queries since the error might be caused from various reasons (i.e. failing connection,
            # database recovery, etc.)
            cache.set(query, results)

        return results

    return wrapper

@cache_query_results
def exponential_backoff_query_execution(sql: str, db: DatabaseSqlite) -> pd.DataFrame:
    """
    Executes the given query in the given db with exponential backoff in case the database is in recovery mode.
    The waiting intervals are: 16, 64, 180, 600 seconds

    Args:
        sql (str): The query to execute.
        db (Database): The database upon which the query will be executed.

    Returns:
        The Dataframe with the results and the execution time of the query.
    """
    wait_intervals = [16, 64, 180, 600]
    for wait_interval in wait_intervals:
        result = db.execute(sql=sql, limit=-1)
        if isinstance(result, pd.DataFrame):
            return result
        elif (
            "error" in result
            and "the database system is in recovery mode" in result["error"]
        ):
            logger.warning(
                f"The database is in recovery mode. Waiting {wait_interval} seconds before trying again."
            )
            time.sleep(wait_interval)
        else:
            return result


def _get_results(
    sql: str, db: DatabaseSqlite, query_type: Literal["reference", "prediction"]
) -> (pd.DataFrame, float):
    """
    Executes the given query in the given db.

    Args:
        sql (str): The query to execute.
        db (Database): The database upon which the query will be executed.
        query_type (str): The type of query to execute. THe possible options are: 'reference', 'prediction'.
            This parameter is used for the messages in case of an error.

    Returns:
        The Dataframe with the results and the execution time of the query.
    """
    start_time = time.time()

    result = exponential_backoff_query_execution(sql=sql, db=db)
    exec_time = time.time() - start_time

    if "error" in result:
        logger.warning(
            f"There was an error while executing the {query_type} query {sql}! The execution accuracy will be set "
            f"to 0."
        )
        raise SyntaxError(result["error"])

    return result, exec_time
def exists_order_by(sql_query: str, dialect: str = "postgres") -> bool:
    """Returns True if there is a order by clause in the query. (Subqueries are not considered)"""
    parsed_sql = parse_one(sql_query, dialect=dialect)

    if parsed_sql is not None:
        return "order" in parsed_sql.args
    else:
        return False
def exec_evaluator(db_name: str, pred: str, target: str) -> dict:
    """
    Returns the execution accuracy result for the given prediction and target sql queries.
    Args:
        db_name: The name of the database upon which the queries will run. It can be one of the 3 cases below:
            * Path of a sqlite database ending in .sqlite or .db
            * A hosted database such as "fc4eosc". For available databases check the utils_configs component
            * A hosted database with a specified schema such as "fc4eosc.fc4eosc_subset"
        pred: The predicted sql query.
        target: The target sql query.
    Returns: A dictionary with the execution accuracy results. (e.g., {"exec": 0, "exec-only_common_result_columns": 1})
    """

    # Connect to database
    if db_name.endswith(".sqlite") or db_name.endswith(".db"):
        # Path of a sqlite database
        db = DatabaseSqlite(db_name)
        db_dialect = "sqlite"


    # Get the results of the predicted query
    pred_results, pred_exec_time = _get_results(
        sql=pred, db=db, query_type="prediction"
    )

    # Get the results of the target query
    target_results, target_exec_time = _get_results(
        sql=target, db=db, query_type="reference"
    )

    # Check if the order of the results matter
    order_matters = exists_order_by(target, db_dialect)

    # Compare the results
    comp = _results_comparison(pred_results, target_results, order_matters)

    exec_results = {
        "exec": 1 if comp == "equal" else 0,
        "exec-only_common_result_columns": 0 if comp == "different" else 1,
        "exec-target_result_columns_subset": (
            1 if comp in ["equal", "columns_subset"] else 0
        ),
        "exec-target_result_columns_superset": (
            1 if comp in ["equal", "columns_superset"] else 0
        ),
        "target_exec_time": target_exec_time,
        "pred_exec_time": pred_exec_time,
    }

    return exec_results

class ExecutionAccuracy(Metric):
    __test__ = False

    def __init__(self):
        super().__init__()
        self.execution_accuracy_results_per_pair = []

    @classmethod
    def name(cls) -> str:
        return "exec"

    def _compute_aggregated_results(
        self, report_only_exec: bool = True
    ) -> Union[dict[str, float], float]:
        if report_only_exec:
            results = sum(
                evaluation["exec"]
                for evaluation in self.execution_accuracy_results_per_pair
            ) / len(self.execution_accuracy_results_per_pair)
        else:
            results = {}
            execution_accuracy_types = list(
                self.execution_accuracy_results_per_pair[0].keys()
            )
            for execution_accuracy_type in execution_accuracy_types:
                if execution_accuracy_type != "error":
                    results[execution_accuracy_type] = sum(
                        evaluation[execution_accuracy_type]
                        for evaluation in self.execution_accuracy_results_per_pair
                    ) / len(self.execution_accuracy_results_per_pair)
        return results

    def update(
        self,
        preds: list[str],
        targets: list[str],
        db_paths: list[str],
    ) -> None:
        """
        Updates the values of the execution accuracy results with the calculations for the given predictions and
        targets.
        Args:
            preds (list[str]): A list with the sql queries predicted
            targets (list[str]): A list with the gold sql queries
            db_paths (list[str]): A list with the sqlite db paths of each prediction-target pair
        """
        for prediction, target, db_path in tqdm(
            zip(preds, targets, db_paths),
            desc="Calculating execution accuracy...",
            total=len(preds),
        ):
            if prediction is None:
                self.execution_accuracy_results_per_pair.append(
                    {
                        "exec": 0,
                        "exec-only_common_result_columns": 0,
                        "exec-target_result_columns_subset": 0,
                        "exec-target_result_columns_superset": 0,
                        "target_exec_time": None,
                        "pred_exec_time": None,
                        "exec_error": None,
                    }
                )
            else:
                try:
                    results = exec_evaluator(
                        db_name=db_path, pred=prediction, target=target
                    )

                    self.execution_accuracy_results_per_pair.append(
                        {
                            **results,
                            "exec_error": None,
                        }
                    )
                except Exception as e:
                    logger.warning(f"Error in execution accuracy calculation: {e}")
                    # TODO get the values dynamically
                    self.execution_accuracy_results_per_pair.append(
                        {
                            "exec": 0,
                            "exec-only_common_result_columns": 0,
                            "exec-target_result_columns_subset": 0,
                            "exec-target_result_columns_superset": 0,
                            "target_exec_time": None,
                            "pred_exec_time": None,
                            "exec_error": str(e),
                        }
                    )

    def compute(
        self, aggregated: bool = True, report_only_exec: bool = True
    ) -> Union[list, dict, float]:
        """
        Returns the execution accuracy for the whole corpus (aggregated or per pair).
        Args:
            aggregated (bool): If True, the average execution accuracy score is returned for all the corpus. Otherwise,
            a dictionary is returned with the execution accuracy results of each pair.
            report_only_exec (bool): If True in the aggregated results only the default execution accuracy is reported.
                Otherwise, the report contains all the available execution accuracy types
                (e.g., execution accuracy with only considering the common result columns)
        """
        if not aggregated:
            return self.execution_accuracy_results_per_pair
        else:
            return self._compute_aggregated_results(report_only_exec=report_only_exec)

def execute_sqlite_query(sql_query: str, sqlite_path: str):
    """
    Returns the result after executing the query in the database

    Args:
        sql_query (str): The query to be executed.
        sqlite_path (str): The path of the database to execute the query.

    Returns:
        pd.DataFrame: The execution result of the given sql_query in the database.
    """
    # Connect to the database and create a cursor
    db_conn = sqlite3.connect(sqlite_path)
    cursor = db_conn.cursor()

    # Execute the query and fetch results
    cursor.execute(sql_query)
    results = pd.DataFrame(cursor.fetchall())

    # Close the cursor and the connection with the database
    cursor.close()
    db_conn.close()

    return results

def insert_types(db: DatabaseSqlite, schema: list) -> None:
    types = db.get_types_of_db()
    for table in schema:
        for column in table["columns"]:
            column["data_type"] = types[table["table_name"]][column["column"]]


def insert_primary_keys(db: DatabaseSqlite, schema: list) -> None:
    pks = db.get_primary_keys()
    for table in schema:
        for column in table["columns"]:
            column["is_pk"] = (
                True if column["column"] in pks[table["table_name"]] else False
            )


def insert_foreign_keys(db: DatabaseSqlite, schema: list) -> None:
    fks = db.get_foreign_keys()
    for table in schema:
        for column in table["columns"]:
            if (
                table["table_name"] not in fks
                or column["column"] not in fks[table["table_name"]]
            ):
                column["foreign_keys"] = []
            else:
                column["foreign_keys"] = fks[table["table_name"]][column["column"]]

def get_list_from_df(df: pd.DataFrame) -> list:
    if df.empty:
        return []
    elif len(df) == 1:
        return [df.squeeze()]
    else:
        return list(df.squeeze())  # df -> series -> list

def detect_special_char(name: str) -> bool:
    """Check if a name has a special character besides alphanumerics and underscores."""
    acceptable_chars = string.ascii_letters + string.digits + "_"

    for char in name:
        if char not in acceptable_chars:
            return True

    return False

def add_quotation_mark(name: str, dialect: str) -> str:
    if not detect_special_char(name):
        # If there are no special characters, return the name as is
        return name
    elif dialect == "sqlite":
        return "`" + name + "`"
    else:
        return '"' + name + '"'
    
def add_quotes_to_names(schema: List[Dict], dialect: str) -> List[Dict]:
    for table in schema:
        # Check the table name
        table["table_name"] = add_quotation_mark(table["table_name"], dialect)
        for column in table["columns"]:
            # Check each column name in the table
            column["column"] = add_quotation_mark(column["column"], dialect)
            for foreign_key in column["foreign_keys"]:
                # Check each foreign key table and column connected to current column
                foreign_key["foreign_table"] = add_quotation_mark(
                    foreign_key["foreign_table"], dialect
                )
                foreign_key["foreign_column"] = add_quotation_mark(
                    foreign_key["foreign_column"], dialect
                )

    return schema

def insert_inferred_foreign_keys(schema: list) -> None:
    """
    Infer foreign keys based on the column names.
    If a column name is the same as another table + column, then it is considered as an fk.
    Names are simplified by:
    * Removing the last 's' if it exists (to avoid plurals)
    * Removing the underscores
    * Lowercasing the name

    Note: This method should be called after insert_foreign_keys which inserts the foreign keys from the database.
    """
    possible_foreign_keys = {}

    def simplify_name(column_name: str, table_name: str = None) -> str:
        if column_name.endswith("s"):
            column_name = column_name[:-1]

        if table_name and table_name.endswith("s"):
            table_name = table_name[:-1]

        ret_name = f"{table_name}_{column_name}" if table_name else column_name
        ret_name = ret_name.lower()
        ret_name = ret_name.replace("_", "")

        return ret_name

    for table in schema:
        for column_info in table["columns"]:
            fk_name = simplify_name(column_info["column"], table["table_name"])

            possible_foreign_keys[fk_name] = {
                "table": table["table_name"],
                "column": column_info["column"],
            }

    for table in schema:
        for column_info in table["columns"]:
            # If the column name is the same as another table + column, then it is considered a foreign key
            column_simple_name = simplify_name(column_info["column"])

            if column_simple_name in possible_foreign_keys:
                foreign_key = possible_foreign_keys[column_simple_name]

                if "foreign_keys" not in column_info:
                    column_info["foreign_keys"] = []

                # Check that the foreign key does not already exist
                fk_exists = False
                for fk in column_info["foreign_keys"]:
                    if (
                        fk["foreign_table"] == foreign_key["table"]
                        and fk["foreign_column"] == foreign_key["column"]
                    ):
                        fk_exists = True
                        break

                # Add the fk in the list of foreign keys
                if not fk_exists:
                    column_info["foreign_keys"].append(
                        {
                            "foreign_table": foreign_key["table"],
                            "foreign_column": foreign_key["column"],
                        }
                    )
                    
def get_sample_values_of_column(
    db:  DatabaseSqlite,
    table_name: str,
    column_name: str,
    sample_size: int,
) -> pd.DataFrame:
    """
    Will return <sample_size> distinct values of the column.

    Args:
        db: The database executor object
        table_name: The table name
        column_name: The column name
        sample_size: The sample size to return

    Returns:
        A tuple of a dataframe with the example values.
    """
    # If there are is_categorical_thresh + 1 distinct values, then the column is not considered categorical
    distinct_values = db.execute(
        f'SELECT DISTINCT "{column_name}" FROM "{table_name}" WHERE "{column_name}" IS NOT NULL',
        limit=sample_size,
    )

    return distinct_values
SCHEMA_CACHE_PATH = str("assets/schema_cache")

def filter_out_system_tables(schema: dict) -> dict:
    schema["tables"] = [
        table
        for table in schema["tables"]
        if "pg_stat_" not in table and "sqlite_" not in table
    ]
    schema["columns"] = [
        column
        for column in schema["columns"]
        if "pg_stat_" not in column and "sqlite_" not in column
    ]

    return schema

def split_list_of_tables_cols(tables_cols: List[str]) -> dict:
    """
    The schema is given in a list of [table1.col1, table1.col2, table2.col1, ...]
    We split this list into a dictionary of tables with their columns

    Args:
        tables_cols: The list of tables and columns

    Returns:
        A dictionary with tables and columns
    """
    tables = defaultdict(list)
    for table_col in tables_cols:
        table, column = table_col.split(".")
        tables[table].append(column)

    return tables

def obtain_schema_from_db(
    db: DatabaseSqlite,
    sample_size: int,
    infer_foreign_keys: bool = False,
) -> list:
    """
    Obtain the schema of the database from the database object. The schema is returned in the following format:
    ```
        [
            {
                "table_name": "table1",
                "columns": [
                    {
                        "column": "col1",
                        "values": [1, 2, 3, 4, 5],
                        "data_type": "INTEGER",
                        "is_pk": False,
                        "foreign_keys": [
                            {
                                "foreign_table": "table2",
                                "foreign_column": col3,
                            },
                            ...
                        ],
                    },
                    ...
                ]
            },
            ...
        ]
        ```

    Args:
        db: The database object
        sample_size: The sample size to return if the column is not categorical
        infer_foreign_keys: Whether to infer foreign keys or not based on the column names

    Returns:
        The schema of the database in the above format
    """
    schema = db.get_tables_and_columns()
    schema = filter_out_system_tables(schema)
    tables_with_cols = split_list_of_tables_cols(schema["columns"])

    ret_schema = []
    for table in tables_with_cols.keys():
        table_dict = {
            "table_name": table,
            "columns": [],
        }
        for column in tables_with_cols[table]:
            example_values = get_sample_values_of_column(db, table, column, sample_size)
            table_dict["columns"].append(
                {
                    "column": column,
                    "values": get_list_from_df(example_values),
                }
            )
        ret_schema.append(table_dict)

    insert_types(db, ret_schema)
    insert_primary_keys(db, ret_schema)
    insert_foreign_keys(db, ret_schema)

    if infer_foreign_keys:
        insert_inferred_foreign_keys(ret_schema)

    # Add quotes in table and column names when necessary
    dialect = "sqlite" if isinstance(db, DatabaseSqlite) else "postgres"
    ret_schema = add_quotes_to_names(ret_schema, dialect)

    return ret_schema

def schema_exists(schema_name: str) -> bool:
    if os.path.exists(f"{SCHEMA_CACHE_PATH}/schemas.json"):
        with open(f"{SCHEMA_CACHE_PATH}/schemas.json", "r") as f:
            schemas = json.load(f)
            return schema_name in schemas
        
def store_schema(schema: list, schema_id: str) -> None:
    if os.path.exists(f"{SCHEMA_CACHE_PATH}/schemas.json"):
        with open(f"{SCHEMA_CACHE_PATH}/schemas.json", "r") as f:
            schemas = json.load(f)
    else:
        if not os.path.exists(SCHEMA_CACHE_PATH):
            os.makedirs(SCHEMA_CACHE_PATH)
        schemas = {}

    schemas[schema_id] = schema

    with open(f"{SCHEMA_CACHE_PATH}/schemas.json", "w") as f:
        json.dump(schemas, f, default=str, indent=2)
     
def get_schema(
    database_str: str,
    sample_size: int = 20,
    infer_foreign_keys: bool = False,
    enable_cache: bool = True,
) -> list:
    """
    Provides the schema of a database given the database string. The database string can an existing database
    i.e. "fc4eosc" or a sqlite database path ending with .db or .sqlite

    Args:
        database_str: The database string
        sample_size: The sample size to return if the column is not categorical
        infer_foreign_keys: Whether to infer foreign keys or not based on the column names
        enable_cache: A boolean to enable caching of the schema. If disable no access of the cache will be performed.

    Returns:
        list: The schema of the database in the following format
            ```
            [
                {
                    "table_name": "table1",
                    "columns": [
                        {
                            "column": "col1",
                            "values": [1, 2, 3, 4, 5],
                            "data_type": "INTEGER",
                            "is_pk": False,
                            "foreign_keys": [
                                {
                                    "foreign_table": "table2",
                                    "foreign_column": col3,
                                },
                                ...
                            ],
                        },
                        ...
                    ]
                },
                ...
            ]
            ```
    """
    # We want to separate schemas with different number of example values
    schema_id = f"{database_str}_{str(sample_size)}_{str(infer_foreign_keys)}"

    if schema_exists(schema_id) and enable_cache:
        with open(f"{SCHEMA_CACHE_PATH}/schemas.json", "r") as f:
            return json.load(f)[schema_id]

    db = DatabaseSqlite(database_str)


    schema = obtain_schema_from_db(db, sample_size, infer_foreign_keys)

    if enable_cache:
        store_schema(schema, schema_id)

    return schema

HUGGINGFACE_REPO = "DARELab/BIRD"
BRANCH = "main"

def unzip_files(root_folder: str, zipped_file_paths: List[str]):
    """
    Unzips the files in the given list of zipped_file_paths in the root directory given by `root_folder`.
    Depending on the file extension, the appropriate unzip function is called.
    Args:
        root_folder (str): The root folder where the zipped files are located.
        zipped_file_paths (List[str]): A list of paths to the zipped files.
    """
    for zipped_file_path in zipped_file_paths:
        if zipped_file_path.endswith(".zip"):
            unzip_zip_files(root_folder, [zipped_file_path])
        elif zipped_file_path.endswith(".gz"):
            unzip_gz_files(root_folder, [zipped_file_path])
        else:
            raise ValueError(
                f"Zipped file extension of file {zipped_file_path} not supported."
            )


def unzip_zip_files(root_folder: str, zipped_file_paths: List[str]):
    for zipped_file_path in zipped_file_paths:
        with zipfile.ZipFile(root_folder + zipped_file_path) as zip_ref:
            zip_ref.extractall(root_folder)


def unzip_gz_files(root_folder: str, zipped_file_paths: List[str]):
    for zipped_file_path in zipped_file_paths:
        file_name = (os.path.basename(zipped_file_path)).rsplit(".", 1)[
            0
        ]  # get file name for file within
        with gzip.open(root_folder + zipped_file_path, "rb") as f_in, open(
            root_folder + file_name, "wb"
        ) as f_out:
            shutil.copyfileobj(f_in, f_out)
            
            
def get_file_from_huggingface_hub(
    repo_id: str,
    cache_dir: str,
    branch: Optional[str] = None,
    post_processing: Optional[List[Tuple[Callable, List[Any]]]] = None,
) -> None:
    """
    Downloads a file from the HuggingFace Hub.

    Args:
        repo_id (str): The repository ID of the file to download (i.e. MikeXydas/iris).
        cache_dir (str): The directory to download the file to.
        branch (Optional[str]): The branch to download the file from.
        post_processing (Optional[List[Tuple[Callable, List[Any]]]]): A list of tuples containing a function and its
            arguments to be applied to the downloaded files (i.e. [(unzip_files, [cache/folder/, [file.zip]]).
    """

    load_dotenv()

    folder_name = repo_id.split("/")[-1]
    cache_dir = cache_dir + "/" if cache_dir[-1] != "/" else cache_dir

    cached_datasets = list(map(os.path.basename, glob.glob(f"{cache_dir}*")))

    if folder_name in cached_datasets:  # Check if file is already downloaded in cache
        return None

    print(branch)

    os.makedirs(cache_dir + folder_name, exist_ok=True)
    snapshot_download(
        repo_id=repo_id,
        revision=branch,
        local_dir=cache_dir + folder_name,
        repo_type="dataset",
        local_dir_use_symlinks=False,
        ignore_patterns=["*.md", ".gitattributes"],
        token=os.getenv("huggingface_access_token"),
    )

    if post_processing is not None:
        for func, args in post_processing:
            func(*args)

class Bird():

    def __init__(self, cache_dir: Optional[str] = None) -> None:
        """
        Initializes the BIRD dataset class.

        Args:
            cache_dir (Optional[str], optional): The cache dir to download (or
                load) the dataset. Defaults to None, which uses the default dir.
        """
        self.data = None
        self.cache_dir = "assets/bird_cache" if cache_dir is None else cache_dir
        self.huggingface_repo = HUGGINGFACE_REPO
        self.dataset_folder = self.cache_dir + HUGGINGFACE_REPO.split("/")[-1] + "/"
        self._schema_cache = {}

    def get_info(self) -> Dict[str, str]:
        """
        Returns a dictionary containing information about the dataset.
        """
        return {
            "name": "BIRD",
            "description": "Big Bench for Large-Scale Database Grounded Text-to-SQLs.",
            "huggingface_repo": HUGGINGFACE_REPO,
            "original_url": "https://yale-lily.github.io/spider",
            "formats": ["json"],  # List of available formats
            "dataset_folder": (
                self.dataset_folder if self.data is not None else "NOT_YET_LOADED"
            ),
        }

    def _init_data(self):
        """
        Downloads the BIRD dataset from Huggingface unzips it, and loads the
        train set, train tables, dev set, and dev tables files.
        """
        get_file_from_huggingface_hub(
            repo_id=HUGGINGFACE_REPO,
            branch=BRANCH,
            cache_dir=self.cache_dir,
            post_processing=[
                (
                    unzip_files,
                    [
                        self.dataset_folder,
                        [
                            "train.zip",
                            "dev.zip",
                            "train/train_databases.zip",
                            "dev/dev_databases.zip",
                        ],
                    ],
                )
            ],
        )

        # Load the train.json file
        train_df = pd.read_json(self.dataset_folder + "train/train.json")
        # Load the train_tables.json file
        with open(self.dataset_folder + "train/train_tables.json", "r") as fp:
            tables_list = json.load(fp)
        train_tables_dict = {
            schema_dict["db_id"]: schema_dict for schema_dict in tables_list
        }

        # Load the dev.json file
        dev_df = pd.read_json(self.dataset_folder + "dev/dev.json")
        # Load the dev_tables.json file
        with open(self.dataset_folder + "dev/dev_tables.json", "r") as fp:
            tables_list = json.load(fp)
        dev_tables_dict = {
            schema_dict["db_id"]: schema_dict for schema_dict in tables_list
        }

        self.data = {
            "train": train_df,
            "train_tables": train_tables_dict,
            "dev": dev_df,
            "dev_tables": dev_tables_dict,
        }

    def _row_to_datapoint(self, row: pd.Series) -> SqlQueryDatapoint:
        # Check if there are multiple NL Queries for this example
        if isinstance(row["question"], list):
            nl_query = row["question"][0]
            nl_query_alt = row["question"][1:]
        else:
            nl_query = row["question"]
            nl_query_alt = None

        return SqlQueryDatapoint(
            nl_query=nl_query,
            nl_query_alt=nl_query_alt,
            sql_query=row["SQL"],
            db_id=row["db_id"],
            db_path=self.get_db_path(row["db_id"]),
            db_schema=self.get_schema(row["db_id"]),
            # NOTE: The line bellow assumes that bird is used for Text-to-SQL only
            # In th future we might want to change this method to `_row_to_text_to_sql_datapoint`
            # and create similar methods for different tasks like `row_to_sql_to_text_datapoint.
            ground_truth=row["SQL"],
            evidence=row["evidence"],
        )

    def get(self) -> Dict[str, List[SqlQueryDatapoint]]:
        """Returns a dictionary with the train and dev splits of the dataset.

        Returns:
            Dict[str, List[SqlQueryDatapoint]]: A dictionary with keys "train" and "dev",
                and the corresponding lists of SqlQueryDatapoint.
        """
        return {
            "train": [
                self._row_to_datapoint(row) for _, row in self.data["train"].iterrows()
            ],
            "dev": [
                self._row_to_datapoint(row) for _, row in self.data["dev"].iterrows()
            ],
        }

    def get_raw(self) -> Dict[str, pd.DataFrame]:
        """
        Returns a dictionary containing the train, dev, and tables json files
        that have been directly loaded as Pandas DataFrames.

        Returns:
            Dict[str, pd.DataFrame]: A dictionary with keys "train" and "dev",
                and the entire corresponding json file loaded as a DataFrame.
        """
        return self.data

    def _check_db_id(self, db_id: str) -> None:
        """
        Checks if the given db_id is valid.

        Args:
            db_id (str): A database id

        Returns:
            None if the db_id is a valid one else raise a FileNotFound exception
        """
        if not (db_id in self.data["train_tables"] or db_id in self.data["dev_tables"]):
            raise FileNotFoundError(f"The requested db_id ({db_id}) does not exist!")

    def get_tables(self, db_id: str) -> pd.DataFrame:
        """
        Returns the tables json object that corresponds to the given db_id.

        Args:
            db_id (str): A database id that should appear in the tables.json file.

        Returns:
            pd.DataFrame: The json object in the tables.json file that corresponds
                to the given db_id.
        """
        if db_id in self.data["train_tables"]:
            return self.data["train_tables"][db_id]
        elif db_id in self.data["dev_tables"]:
            return self.data["dev_tables"][db_id]
        else:
            raise FileNotFoundError(f"The requested db_id ({db_id}) does not exist!")

    def get_schema(self, db_id: str) -> List:
        """Returns the schema of the database named `db_id`.
        Loads the SQLite database and automatically extracts the schema.

        Args:
            db_id (str): The id of the requested database.

        Returns:
            List: The schema as a list of the DB tables.
        """
        # Get cached schema if it exists
        schema = self._schema_cache.get(db_id, None)
        # If schema was not found, then create it and store it in the cache
        if schema is None:
            # Obtain schema automatically from SQLite file
            db_path = self.get_db_path(db_id)
            # NOTE: We might want to experiment with the value parameters in the future
            schema = get_schema(
                db_path, sample_size=20, infer_foreign_keys=True
            )

            # Add notes to schema
            self.add_notes_in_schema(schema, db_id)

            self._schema_cache[db_id] = schema

        return schema

    def get_db_path(self, db_id: str) -> str:
        """
        Returns the database path that corresponds to the given db_id.

        Args:
            db_id (str): A database id that should appear in the tables.json file.
        """
        if db_id in self.data["train_tables"]:
            return f"{self.dataset_folder}train_databases/{db_id}/{db_id}.sqlite"
        elif db_id in self.data["dev_tables"]:
            return f"{self.dataset_folder}dev_databases/{db_id}/{db_id}.sqlite"
        else:
            raise FileNotFoundError(f"The requested db_id ({db_id}) does not exist!")

    def execute_query(self, sql_query: str, db_id: str) -> pd.DataFrame:
        """
        Returns the result after executing the query in the database

        Args:
            sql_query (str): The query to be executed.
            db_id (str): The id of the database to execute the query.

        Returns:
            pd.DataFrame: The execution result of the given sql_query in the database that
            corresponds to the given db_id.
        """
        # Get the path of the sqlite file
        sqlite_path = self.get_db_path(db_id)

        return execute_sqlite_query(sql_query=sql_query, sqlite_path=sqlite_path)

    def add_notes_in_schema(self, schema: List, db_id: str) -> None:
        """
        Adds notes to the schema of the database that corresponds to the given db_id. It will add as a note more
        descriptive column/table names only if there is significant difference between the original column/table names
        and the more descriptive ones.

        Args:
            schema (List): The schema obtained by the get_schema method.
            db_id (str): The id of the database to get the notes from.
        """
        table_info = self.get_tables(db_id)

        table_notes = dict(
            zip(table_info["table_names_original"], table_info["table_names"])
        )

        db_info_path = (
            "/".join(self.get_db_path(db_id).split("/")[:-1]) + "/database_description/"
        )

        def has_significant_difference(original_name, new_name):
            return not (
                original_name.lower().replace("_", "").replace(" ", "")
                == new_name.lower().replace("_", "").replace(" ", "")
            )

        for table in schema:
            try:
                original_table_name = table["table_name"].strip("`")
                note_table_name = table_notes[table["table_name"]]

                if original_table_name in table_notes:
                    table["note"] = (
                        table_notes[original_table_name]
                        if has_significant_difference(
                            original_table_name, note_table_name
                        )
                        else None
                    )

                    aliases = pd.read_csv(db_info_path + f"{original_table_name}.csv")

                    for column in table["columns"]:
                        original_column_name = column["column"].strip("`")
                        column_description = aliases[
                            aliases["original_column_name"].str.strip()
                            == original_column_name
                        ]["column_name"].iloc[0]

                        if (
                            column_description
                            and type(column_description) == str
                            and column_description != ""
                            and has_significant_difference(
                                original_column_name, column_description
                            )
                        ):
                            column["note"] = column_description
            except (KeyError, UnicodeError, FileNotFoundError) as e:
                logger.warning(
                    f"While adding notes for table {table['table_name']}, the following error occurred: {e}"
                )
                table["note"] = None
                for column in table["columns"]:
                    column["note"] = None


def _add_execution_accuracy_metric(results: pd.DataFrame) -> None:
    logger.info(f"Calculating execution accuracy...")
    exec_acc = ExecutionAccuracy()
    exec_acc.update(
        preds=results["predicted_sql_query"].values.tolist(),
        targets=results["sql_query"].values.tolist(),
        db_paths=results["db_path"].values.tolist(),
    )
    exec_results = exec_acc.compute(aggregated=False, report_only_exec=False)
    
    exec_results = pd.DataFrame.from_records(exec_results)
    for column in exec_results.columns:
        results[column] = exec_results[column]


def _calculate_total_statistics(results: pd.DataFrame) -> pd.DataFrame:
    return pd.DataFrame(
        [
            {
                "AVG exec": results["exec"].mean(),
                "AVG exec-only_common_result_columns": results[
                    "exec-only_common_result_columns"
                ].mean(),
                "AVG exec-target_result_columns_subset": results[
                    "exec-target_result_columns_subset"
                ].mean(),
                "AVG exec-target_result_columns_superset": results[
                    "exec-target_result_columns_superset"
                ].mean(),
                "Total exec errors": results["exec_error"].count(),
                "AVG structural_match": results["structural_match"].mean(),
                "AVG operator_match": results["operator_match"].mean(),
                "AVG variable_match": results["variable_match"].mean(),
            }
        ]
    )




def _get_dataset(dataset_name: str, download_dir: Optional[str] = None):
    #ensure that download_dir ens with slash
    if download_dir :
        download_dir = download_dir if download_dir.endswith("/") else download_dir + "/"
    if "." in dataset_name:
        dataset_name, split = dataset_name.split(".")
        dataset_class = Bird
        # Only pass cache_dir to Dataset subclasses
        instance = dataset_class(cache_dir=download_dir)
        return instance.get()[split]
    else:
        dataset_class = Bird
        instance = dataset_class(cache_dir=download_dir)
        dataset = instance.get()
        if type(dataset) is dict:
            unified_dataset = []
            for split, split_dataset in dataset.items():
                unified_dataset.extend(split_dataset)
            return unified_dataset
        else:
            return dataset

def ollama_experiment_runner(config: ExperimentConfig) -> None:
    """
    Retrieves the SQL predictions and calculates evaluation metrics (e.g., execution
    accuracy) and statistics (e.g., execution time) for the given run configuration.

    Args:
        config: The configuration parameters of the experiment.
    """

    dataset = _get_dataset(config.evaluated_dataset, download_dir=config.download_dir)
    start = time.time()
    i = 0
    results = []
    for datapoint in tqdm(dataset, desc="Get model predictions..."):
        handle_text_to_sql(
            datapoint=datapoint,
            config={
                "model": config.model.model_dump(mode="json"),
                "prompt": config.prompt.model_dump(mode="json"),
            },
        )
        if i < 3 :
            print(datapoint.model_input)
            i += 1
        results.append(
            {
                "nl_question": datapoint.nl_query,
                "sql_query": datapoint.sql_query,
                "db_path": datapoint.db_path,
                "predicted_sql_query": datapoint.prediction,
                "model_input": datapoint.model_input,
                "generated_text": datapoint.model_output,
            }
        )
    results = pd.DataFrame(results)

    _add_execution_accuracy_metric(results)


    total_statistics = _calculate_total_statistics(results)

    # SAVE RESULTS
    #displat the total statistics dataframe
    print(total_statistics)

    logger.info(f">>> Experiment finished in {time.time() - start} seconds.")
    
if __name__ == "__main__":
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    logging.basicConfig(
        filename='logs/text.log',
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    model_names = ["qwen2.5-coder:7b","qwen2.5-coder:32b","llama3.1:70b", "llama3.3:70b", "phi4"]
    value_links_files = ["none","assets/precision_1_per_query.json","assets/precision_0_5_per_query.json","assets/precision_0_3_per_query.json"
                         ,"assets/precision_0_2_per_query.json","assets/precision_0_1_per_query.json","assets/chess.json","assets/codes.json"]
    for model_name in model_names:
        experiment_config = ExperimentConfig(
            model=ExperimentConfigModel(
                name=model_name,
                options={
                    "temperature": 0,
                    "num_ctx": 8192
                },
                mode="generate"
            ),
            prompt=ExperimentConfigPrompt(
                version="zero_shot_prompt",
                database_representation=ExperimentConfigDbRepresentation(
                    format="compact",
                    example_values=ExperimentDbRepresentationExampleValues(
                        num=0,
                        categorical_threshold=0
                    ),
                    include_primary_keys=True,
                    include_foreign_keys=True,
                    include_notes=True,
                    value_linking_path=None
                )
            ),
            evaluated_dataset="Bird.dev",
            download_dir="assets/datasets_text_to_sql"
        )
        ollama_experiment_runner(experiment_config)
    for model_name in model_names:
        for value_links_file in value_links_files:
            experiment_config = ExperimentConfig(
                model=ExperimentConfigModel(
                    name=model_name,
                    options={
                        "temperature": 0,
                        "num_ctx": 8192
                    },
                    mode="generate"
                ),
                prompt=ExperimentConfigPrompt(
                    version="zero_shot_prompt",
                    database_representation=ExperimentConfigDbRepresentation(
                        format="compact",
                        example_values=ExperimentDbRepresentationExampleValues(
                            num=2,
                            categorical_threshold=1
                        ),
                        include_primary_keys=True,
                        include_foreign_keys=True,
                        include_notes=True,
                        value_linking_path=value_links_file if value_links_file != "none" else None
                    )
                ),
                evaluated_dataset="Bird.dev",
                download_dir="assets/datasets_text_to_sql"
            )
            ollama_experiment_runner(experiment_config)
    